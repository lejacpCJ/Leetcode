# 1948. Delete Duplicate Folders in System

## Problem Info

- **Difficulty:** Hard
- **Topics:** Array, Hash Table, Tree, Depth-First Search, String, Trie
- [Link to the problem](https://leetcode.com/problems/delete-duplicate-folders-in-system/)

## Description

Due to a bug, there are many duplicate folders in a file system. You are given a 2D array `paths`, where `paths[i]` is an array representing an absolute path to the ith folder in the file system.

For example, `["one", "two", "three"]` represents the path `"/one/two/three"`.

Two folders (not necessarily on the same level) are identical if they contain the same non-empty set of identical subfolders and underlying subfolder structure. The folders do not need to be at the root level to be identical. If two or more folders are identical, then mark the folders as well as all their subfolders.

Once all the identical folders and their subfolders have been marked, the file system will delete all of them. The file system only runs the deletion once, so any folders that become identical after the initial deletion are not deleted.

Return the 2D array `ans` containing the paths of the remaining folders after deleting all the marked folders. The paths may be returned in any order.

## Example

Input: `paths = [["a"],["c"],["d"],["a","b"],["c","b"],["d","a"]]`  
Output: `[["d"],["d","a"]]`  
Explanation: Folders "/a" and "/c" (and their subfolders) are marked for deletion because they both contain an empty folder named "b".


## Constraints

- `1 <= paths.length <= 2 * 10^4`
- `1 <= paths[i].length <= 500`
- `1 <= paths[i][j].length <= 10`
- `1 <= sum(paths[i][j].length) <= 2 * 10^5`
- `path[i][j]` consists of lowercase English letters.
- No two paths lead to the same folder.
- For any folder not at the root level, its parent folder will also be in the input.

---

## Solution

We can model the file system as a tree (Trie). To find duplicate folders, we serialize each subtree and use a hash map to count how many times each serialization appears. If a subtree serialization appears more than once, we mark all such subtrees for deletion.

**Steps:**
1. Build a Trie representing the folder structure.
2. Serialize each subtree using post-order traversal, and count the frequency of each serialization.
3. In a second traversal, mark nodes (folders) for deletion if their serialization is duplicated.
4. Collect all remaining paths using DFS.

**Python Code:**

```python
from typing import List, Dict
from collections import defaultdict

class TrieNode:
    def __init__(self):
        self.children = dict()
        self.name = ""
        self.deleted = False

def insert(root: TrieNode, path: List[str]):
    node = root
    for folder in path:
        if folder not in node.children:
            node.children[folder] = TrieNode()
            node.children[folder].name = folder
        node = node.children[folder]

def serialize(node: TrieNode, counter: Dict[str, int], lookup: Dict[str, List[TrieNode]]) -> str:
    if not node.children:
        return ""
    serials = []
    for child in sorted(node.children):
        serials.append(child + "(" + serialize(node.children[child], counter, lookup) + ")")
    serial = "".join(serials)
    counter[serial] += 1
    lookup[serial].append(node)
    return serial

def mark_deleted(counter: Dict[str, int], lookup: Dict[str, List[TrieNode]]):
    for serial, count in counter.items():
        if count > 1:
            for node in lookup[serial]:
                node.deleted = True

def collect(node: TrieNode, path: List[str], res: List[List[str]]):
    for child in node.children.values():
        if not child.deleted:
            res.append(path + [child.name])
            collect(child, path + [child.name], res)

class Solution:
    def deleteDuplicateFolder(self, paths: List[List[str]]) -> List[List[str]]:
        root = TrieNode()
        for path in paths:
            insert(root, path)
        counter = defaultdict(int)
        lookup = defaultdict(list)
        serialize(root, counter, lookup)
        mark_deleted(counter, lookup)
        res = []
        collect(root, [], res)
        return res
```

**Complexity:**  
- Time: O(N * L), where N is the number of paths and L is the average path length.
- Space: O(N * L), for the Trie and serialization storage.